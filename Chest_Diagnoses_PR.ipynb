{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Chest Diagnoses_PR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rodaina1997/Chest-diagnosis/blob/main/Chest_Diagnoses_PR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smEvfhgXUszm"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQReuRQLn0i9",
        "outputId": "a13976d8-83fd-46b4-8981-ace948b8cb5c"
      },
      "source": [
        "pip install split-folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split-folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIdNLH0zn60T",
        "outputId": "5f3e1598-54c2-4ab5-e8fc-3adfb00d77b7"
      },
      "source": [
        "pip install split-folders tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.6/dist-packages (0.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2XteCOlySvH"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "import numba\n",
        "from numba import cuda\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import modules, Linear, ReLU,MaxPool2d\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#importing data from google drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import PIL \n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import splitfolders "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhb2YOZIieyb"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Y3j3XqimIH"
      },
      "source": [
        "**Loading files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CWtnGXmUTIX",
        "outputId": "3d112d26-6d74-4207-d2cc-e500bc179364"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJkw5IEfZQYL"
      },
      "source": [
        "\n",
        "dataset_classes = {\n",
        "    'Normal': '/content/drive/MyDrive/Pattern_Recognition/COVID-19_Radiography_Database/Normal',\n",
        "    'Viral': '/content/drive/MyDrive/Pattern_Recognition/COVID-19_Radiography_Database/Viral Pneumonia',\n",
        "    'COVID': '/content/drive/MyDrive/Pattern_Recognition/COVID-19_Radiography_Database/COVID'\n",
        "}\n",
        "dataset_path ='/content/drive/MyDrive/Pattern_Recognition/COVID-19_Radiography_Database'\n",
        "CLASSES = ['COVID','NORMAL' ,'Viral Pneumonia']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCbxBMmboQk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e35013b-000e-415d-8e32-6e290d14f546"
      },
      "source": [
        "splitfolders.ratio(dataset_path, output=\"/content/output\", seed=1337, ratio=(.75, .1,.15), group_prefix=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 3886 files [41:55,  1.55 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QPZoH5qpyfX"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [#transforms.ToPILImage(),\n",
        "     transforms.Resize(size=(256,256)),\n",
        "     transforms.ToTensor(),\n",
        "     #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "     ]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PchzgAbjpzbX"
      },
      "source": [
        "train_data = torchvision.datasets.ImageFolder(root='/content/output/train', transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_data = torchvision.datasets.ImageFolder(root='/content/output/test', transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "validate_data = torchvision.datasets.ImageFolder(root='/content/output/val', transform=transform)\n",
        "valloader = torch.utils.data.DataLoader(validate_data, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH3FFhb7tm75"
      },
      "source": [
        "**Reading files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9OID0UaOohO"
      },
      "source": [
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_models(model,trainloader,valloader,testloader):\n",
        "  num_epochs = 5\n",
        "  learning_rate = 0.0001\n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=5, threshold=1e-4)\n",
        "\n",
        "  # Train the model\n",
        "  total_step = len(train_loader)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for epoch in range(num_epochs):\n",
        "      epoch_loss = 0\n",
        "      epoch_acc = 0\n",
        "      for i, (images, labels) in enumerate(trainloader):  \n",
        "          # Move tensors to the configured device\n",
        "          images = images.view(-1, 256*256).to(device) #### THIS LINE (1,256,256) = > (256,256)\n",
        "          labels = labels.to(device)\n",
        "        \n",
        "          # Forward pass\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "          # Backprpagation and optimization  \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "          correct_results_sum = (predicted == labels).sum().float()\n",
        "          acc = correct_results_sum/labels.shape[0]\n",
        "          acc = torch.round(acc * 100)\n",
        "          \n",
        "          epoch_acc += acc.item()\n",
        "\n",
        "          f1score = f1_score(labels, predicted, average='micro')\n",
        "          \n",
        "      print ('Epoch [{}/{}], Loss: {:.4f}, Acc: {:.4f}' \n",
        "            .format(epoch+1, num_epochs,epoch_loss/total_step, epoch_acc/total_step))\n",
        "                      \n",
        "  print(f'Accuracy (Training) : {format(100 * correct / total)}% ---> F1 score: {round(f1score, 4)}')\n",
        "\n",
        "  # Test the model\n",
        "  # In the test phase, don't need to compute gradients (for memory efficiency)\n",
        "  with torch.no_grad():\n",
        "      correct1 = 0\n",
        "      total1 = 0\n",
        "      correct2 = 0\n",
        "      total2 = 0\n",
        "      for images1, labels1 in validation_loader:\n",
        "          images1 = images1.reshape(-1, 256*256).to(device)\n",
        "          labels1 = labels1.to(device)\n",
        "          outputs1 = model(images1)\n",
        "          _, predicted1 = torch.max(outputs1.data, 1)\n",
        "          total1 += labels1.size(0)\n",
        "          correct1 += (predicted1 == labels1).sum().item()\n",
        "          f1score1 = f1_score(labels1, predicted1, average='micro')\n",
        "\n",
        "      print(f'Accuracy (Validation) : {format(100 * correct1 / total1)}% ---> F1 score: {round(f1score1, 4)}')\n",
        "      \n",
        "      for images2, labels2 in test_loader:\n",
        "          images2 = images2.reshape(-1, 256*256).to(device)\n",
        "          labels2 = labels2.to(device)\n",
        "          outputs2 = model(images2)\n",
        "          _, predicted2 = torch.max(outputs2.data, 1)\n",
        "          total2 += labels2.size(0)\n",
        "          correct2 += (predicted2 == labels2).sum().item()\n",
        "          f1score2 = f1_score(labels2, predicted2, average='micro')\n",
        "\n",
        "      print(f'Accuracy (Testing) : {format(100 * correct2 / total2)}% ---> F1 score: {round(f1score2, 4)}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "gMwZdyivp9EM",
        "outputId": "fa557c15-998e-4a56-f31c-bb04ecc763d7"
      },
      "source": [
        "#not used\n",
        "Data =[]\n",
        "label=[]\n",
        "\n",
        "folders = glob.glob(dataset_path+'/*')\n",
        "for folder in folder:\n",
        "  label.append(folder)\n",
        "  for path in glob.glob(folder+'/*'):\n",
        "    Data.append(path)\n",
        "  \n",
        "\n",
        "#for image_path in Data:\n",
        "  #label.append(Data.index(image_path))\n",
        "\n",
        "print(len(label))\n",
        "print(len(Data))\n",
        "print(Data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c78033104982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'folder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGxCKqFvYq0X"
      },
      "source": [
        "**Investigate the shape of images.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlAT7HhyWNpz"
      },
      "source": [
        "#it is not nessesary to run it , it will take much time  \n",
        "shapes=[]\n",
        "for img_shape in Data:\n",
        "  image=cv2.imread(img_shape)\n",
        "  shapes.append(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAipkh8Wx9jx",
        "outputId": "adf4688e-689f-42f7-ec71-f681fe8f962b"
      },
      "source": [
        "\n",
        "# function to get unique values \n",
        "def unique(list1): \n",
        "    x = np.array(list1) \n",
        "    print(np.unique(x)) \n",
        "\n",
        "print(shapes)\n",
        "unique(shapes) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (1024, 1024, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (256, 256, 3), (992, 1192, 3), (891, 1084, 3), (1047, 1275, 3), (852, 1039, 3), (891, 1084, 3), (928, 1130, 3), (256, 256, 3), (1047, 1275, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (891, 1084, 3), (256, 256, 3), (952, 1184, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (160, 187, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (891, 1084, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (197, 253, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (891, 1084, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (891, 1084, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (891, 1084, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (947, 1192, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (1047, 1275, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (891, 1084, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (891, 1084, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (891, 1084, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (917, 1116, 3), (331, 331, 3), (952, 1184, 3), (891, 1084, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (913, 1102, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (891, 1084, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (952, 1184, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (891, 1084, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (917, 1116, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (891, 1084, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (992, 1192, 3), (256, 256, 3), (913, 1108, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (952, 1184, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (256, 256, 3), (256, 256, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (1053, 1053, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (952, 1184, 3), (992, 1192, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (891, 1084, 3), (256, 256, 3), (1053, 1053, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (952, 1184, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (917, 1116, 3), (875, 1108, 3), (331, 331, 3), (331, 331, 3), (891, 1084, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (723, 594, 3), (992, 1192, 3), (256, 256, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (917, 1116, 3), (331, 331, 3), (891, 1084, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (331, 331, 3), (256, 256, 3), (331, 331, 3), (1047, 1275, 3), (952, 1184, 3), (992, 1192, 3), (992, 1192, 3), (891, 1084, 3), (891, 1084, 3), (1047, 1275, 3), (952, 1184, 3), (1047, 1275, 3), (952, 1184, 3), (1053, 1053, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (891, 1084, 3), (1053, 1053, 3), (256, 256, 3)]\n",
            "[   3  160  187  197  253  256  331  594  723  852  875  891  913  917\n",
            "  928  947  952  992 1024 1039 1047 1053 1084 1102 1108 1116 1130 1184\n",
            " 1192 1275]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zlOwzgWjw_j"
      },
      "source": [
        "**Resizing All the Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuAsmHFppZ0b"
      },
      "source": [
        "# not used now\n",
        "def resizing(img,hieght,width):\n",
        "  dim = (hieght,width)\n",
        "  img=np.uint8(img)\n",
        "  #img = np.expand_dims(img, axis=-1)\n",
        "  return cv2.resize(img,dim,interpolation = cv2.INTER_AREA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czJQS2iHgNG5"
      },
      "source": [
        "**Invistgate Different Augmintaion method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XITqBiLIg2jt"
      },
      "source": [
        "**Split data training, validation, and testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHaEnS_qhDSu",
        "outputId": "11c29b85-3fb1-484d-b5dc-2980c048df32"
      },
      "source": [
        "#not used\n",
        "train_ratio = 0.70\n",
        "validation_ratio = 0.10\n",
        "test_ratio = 0.20\n",
        "\n",
        "#x > Data\n",
        "#y > Labels\n",
        "print(len(Data))\n",
        "# train is now 75% of the entire data set\n",
        "# the _junk suffix means that we drop that variable completely\n",
        "x_train, x_test, y_train, y_test = train_test_split(Data, label, test_size=1-train_ratio,random_state=1)\n",
        "\n",
        "# test is now 10% of the initial data set\n",
        "# validation is now 15% of the initial data set\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio),random_state=1) \n",
        "\n",
        "print(len(x_train), len(y_train), len(x_test))\n",
        "\n",
        "# Link :https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
        "# startify issue: https://github.com/davidsbatista/text-classification/issues/1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3897\n",
            "2727 2727 780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVgaqD09BNIU"
      },
      "source": [
        "**My DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulUXsvgYBRYx"
      },
      "source": [
        "#not used\n",
        "class my_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, labels, transform = None):\n",
        "        self.labels   = labels\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        i = self.images[index]\n",
        "        x =PIL.Image.open(i).convert('RGB')\n",
        "        #x=resizing(img ,331,331)\n",
        "        #x=cv2.resize(x,(64,64))\n",
        "        # print(len(x))\n",
        "    \n",
        "        y = self.labels[index]\n",
        "       # y = tuple(y)\n",
        "\n",
        "        \n",
        "        if self.transform != None:\n",
        "            x = self.transform(x)\n",
        "    \n",
        "        return x,y\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKW-3LpAHbRP"
      },
      "source": [
        "#not used\n",
        "transform = transforms.Compose(\n",
        "    [#transforms.ToPILImage(),\n",
        "     transforms.Resize(size=(256,256)),\n",
        "     transforms.ToTensor(),\n",
        "     #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "     ]) \n",
        "  \n",
        "\n",
        "trainset = my_Dataset(x_train, y_train, transform=transform)\n",
        "testset  = my_Dataset(x_test, y_test, transform=transform)\n",
        "valset   = my_Dataset(x_val, y_val, transform=transform)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfJkbR-9ZVyX",
        "outputId": "c0ad69b9-5e66-4dcf-9f6d-efbeeccb6fb3"
      },
      "source": [
        " #not used\n",
        " #print(trainset)\n",
        "x,t=trainset[1998]\n",
        "print(x.shape)\n",
        "print(type(t))\n",
        "print(type(x))\n",
        "print(x.min(),x.max())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n",
            "<class 'tuple'>\n",
            "<class 'torch.Tensor'>\n",
            "tensor(0.) tensor(0.9647)\n",
            "Given list A:  ['Mon', 2, 'Tue', 3]\n",
            "The tuple is :  ('Mon', 2, 'Tue', 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfMY4t3My5Nm"
      },
      "source": [
        "**Data Loaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "NTlrvWiEJNjz",
        "outputId": "d6625096-8611-4185-fae5-a1e6adb9881e"
      },
      "source": [
        "#not used\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "print(len(train_loader))\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                          shuffle=False, num_workers=0)\n",
        "print(len(test_loader))\n",
        "val_loader = torch.utils.data.DataLoader(valset, batch_size=32,\n",
        "                                          shuffle=False, num_workers=0)\n",
        "print(len(val_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-46b88147d071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#not used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                           shuffle=True, num_workers=0)\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m test_loader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyMfScxgbJGG"
      },
      "source": [
        "#not used\n",
        "for X, y in train_loader:\n",
        "    # (X, y) is a mini-batch (small subset of the dataset)\n",
        "    # X.shape = NCHW\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "WEHFqRVC6Lc2",
        "outputId": "6d0ffba9-93be-4af5-f97a-5b8b4d8ef594"
      },
      "source": [
        "#not used\n",
        "plt.imshow(X[2][0],cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4877352cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcoQvi1Pu903"
      },
      "source": [
        "# **Basic Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE5JZ7tIPB5A"
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, hidden_size) \n",
        "        self.fc5 = nn.Linear(hidden_size, hidden_size) \n",
        "        self.fc6 = nn.Linear(hidden_size, num_classes)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "aDDOhMdKPLgG",
        "outputId": "62966b01-b4bb-4f25-d722-bb5264665d8c"
      },
      "source": [
        "input_size = 256*256\n",
        "hidden_size = 256\n",
        "num_classes = 10\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).cuda\n",
        "train_models(model,trainloader,valloader,testloader)\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-970335f27879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Save the model checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f7ae2490140c>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(model, trainloader, valloader, testloader)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mschedular\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'parameters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "9oIcCmWRChid",
        "outputId": "de332ed4-bdb5-4a23-816a-272a70631019"
      },
      "source": [
        "# Build the model\n",
        "model = modules.Sequential(Linear(256*256, 16),\n",
        "                            ReLU(),\n",
        "                            Linear(16, 16),\n",
        "                            ReLU(),\n",
        "                            Linear(16, 3)\n",
        "                            )\n",
        "\n",
        "# Move it to gpu\n",
        "model = model.cuda()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "print_every = 100\n",
        "\n",
        "# Optimization loop\n",
        "for epoch in range(30):\n",
        "    print('Start of epoch', epoch)\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    total_correct = 0\n",
        "    \n",
        "    for iteration, (X, y) in enumerate(trainloader):\n",
        "        # Forward\n",
        "        X = X.reshape(-1, 256*256).cuda()\n",
        "        y = y.cuda()\n",
        "        scores = model(X)\n",
        "        loss = torch.nn.functional.cross_entropy(scores, y)\n",
        "\n",
        "        # Accuracy\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        num_correct = torch.sum(preds == y)\n",
        "        \n",
        "        # Take average\n",
        "        total_loss += loss.item()\n",
        "        total_correct += num_correct.item()\n",
        "        total_examples += len(X)\n",
        "        if iteration % print_every == 0:\n",
        "            print(total_loss / total_examples, total_correct / total_examples)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update\n",
        "        with torch.no_grad():\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-521e9d51bc47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 2262\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (96) to match target batch_size (32)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrfTUbjP2Mpg"
      },
      "source": [
        "# Initialize\n",
        "# To execute all operations on GPU, move both data and network parameters to GPU.\n",
        "\n",
        "\n",
        "#                                   Din, Dout\n",
        "w1 = torch.nn.Parameter(torch.randn(256*256, 256).cuda())\n",
        "w2 = torch.nn.Parameter(torch.randn(256, 256).cuda())\n",
        "w3 = torch.nn.Parameter(torch.randn(256, 3).cuda())\n",
        "\n",
        "#                                  Dout\n",
        "b1 = torch.nn.Parameter(torch.randn(256).cuda())\n",
        "b2 = torch.nn.Parameter(torch.randn(256).cuda())\n",
        "b3 = torch.nn.Parameter(torch.randn(256).cuda())\n",
        "\n",
        "step = 1\n",
        "print_every = 100\n",
        "\n",
        "# Optimization loop\n",
        "for epoch in range(30):\n",
        "    print('Start of epoch', epoch)\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    # for X, y in train_loader:\n",
        "    for iteration, (X, y) in enumerate(trainloader):\n",
        "        # Forward\n",
        "        X = X.reshape(-1, 256*256).cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        a1 = torch.sigmoid(X @ w1 + b1)\n",
        "        a2 = torch.sigmoid(a1 @ w2 + b2)\n",
        "        scores = a2 @ w3 + b3\n",
        "        loss = torch.nn.functional.cross_entropy(scores, y)\n",
        "\n",
        "        # Accuracy\n",
        "        preds = torch.argmax(scores, dim=1)    # scores: (N, c) ==> (N,)\n",
        "        num_correct = torch.sum(preds == y)\n",
        "        \n",
        "        # Take average\n",
        "        total_loss += loss.item\n",
        "        total_correct += num_correct.item()\n",
        "        total_examples += len(X)\n",
        "        if iteration % print_every == 0:\n",
        "            print(total_loss / total_examples, total_correct / total_examples)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update\n",
        "        with torch.no_grad():\n",
        "            w1 -= step * w1.grad\n",
        "            w2 -= step * w2.grad\n",
        "            w3 -= step * w3.grad\n",
        "            b1 -= step * b1.grad\n",
        "            b2 -= step * b2.grad\n",
        "            b3 -= step * b3.grad\n",
        "            \n",
        "            w1.grad.zero_()\n",
        "            w2.grad.zero_()\n",
        "            w3.grad.zero_()\n",
        "            b1.grad.zero_()\n",
        "            b2.grad.zero_()\n",
        "            b3.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM4yBVj6c54s"
      },
      "source": [
        "# CNNManyLayers\n",
        "**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvyvSzyJG7RB"
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "# Model's abstraction\n",
        "class CnnManyLayers(nn.Module):\n",
        "      def __init__(self):\n",
        "        super(CnnManyLayers, self).__init__()\n",
        "        # convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6,16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 256, 5)\n",
        "\n",
        "        self.pool=nn.MaxPool2d(2,2)\n",
        "        \n",
        "        # dropout layers to prevent overfitting\n",
        "        #self.dropout1 = nn.Dropout(0.25)\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        #self.dropout3 = nn.Dropout(0.75)\n",
        "\n",
        "        # fully connected layer\n",
        "        self.fc1 = nn.Linear(28*28*256, 256).cuda()\n",
        "        self.fc2 = nn.Linear(256,120).cuda()\n",
        "        self.fc3 = nn.Linear(120, 84).cuda()\n",
        "        self.fc4 = nn.Linear(84, 3) .cuda()\n",
        "\n",
        "      def forward(self,x):\n",
        "         x=self.conv1(x) #featutres exctraction \n",
        "         x=F.relu(x)\n",
        "         x= self.pool(x) #to reduce image size,cost,avoid overfitting and enhancement of features extraction\n",
        "         #print(x.shape)\n",
        "\n",
        "         x=self.conv2(x)  \n",
        "         x=F.relu(x)\n",
        "         x= self.pool(x)\n",
        "         #print(x.shape)\n",
        "\n",
        "         x=self.conv3(x)  \n",
        "         x=F.relu(x)\n",
        "         x= self.pool(x)\n",
        "         #print(x.shape)\n",
        "        \n",
        "         #x=self.dropout1(x) #to avoid overfitting\n",
        "         x=torch.flatten(x,1) #to get a vector output as input to the fully connected layer ,neurons must be flattened not spatial\n",
        "\n",
        "         x=self.fc1(x) # 1st fully connected layer for classification\n",
        "         x=F.relu(x)\n",
        "         #print(x.shape)\n",
        "\n",
        "         #x=self.dropout2(x)\n",
        "\n",
        "         x=self.fc2(x)\n",
        "         x=F.relu(x)\n",
        "         #print(x.shape)\n",
        "\n",
        "         #x=self.dropout3(x)\n",
        "\n",
        "         x=self.fc3(x)\n",
        "         x=F.relu(x)\n",
        "         #print(x.shape)\n",
        "\n",
        "         #x=self.dropout4(x)\n",
        "\n",
        "         x=self.fc4(x)\n",
        "         #print(x.shape)\n",
        "         output=F.log_softmax(x,dim=1) #log softmax activation function to get probablity of each class\n",
        "         return output\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpBsYxyWpQRN"
      },
      "source": [
        "# CnnFewLayers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz9c3qfipwps"
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "# Model's abstraction\n",
        "class CnnFewLayers(nn.Module):\n",
        "      def __init__(self):\n",
        "        super(CnnFewLayers, self).__init__()\n",
        "        # convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6,16, 5)\n",
        "        \n",
        "        self.pool=nn.MaxPool2d(2,2)\n",
        "        \n",
        "        # dropout layers to prevent overfitting\n",
        "        #self.dropout1 = nn.Dropout(0.25)\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        #self.dropout3 = nn.Dropout(0.75)\n",
        "\n",
        "        # fully connected layer\n",
        "        self.fc1 = nn.Linear(61*61*16, 120).cuda()\n",
        "        self.fc2 = nn.Linear(120,84).cuda()\n",
        "        self.fc3 = nn.Linear(84, 3).cuda()\n",
        "        \n",
        "\n",
        "      def forward(self,x):\n",
        "         x=self.conv1(x) #featutres exctraction \n",
        "         x=F.relu(x)\n",
        "         x= self.pool(x) #to reduce image size,cost,avoid overfitting and enhancement of features extraction\n",
        "         #print(x.shape)\n",
        "\n",
        "         x=self.conv2(x)  \n",
        "         x=F.relu(x)\n",
        "         x= self.pool(x)\n",
        "         #print(x.shape)\n",
        "\n",
        "        \n",
        "         #x=self.dropout1(x) #to avoid overfitting\n",
        "         x=torch.flatten(x,1) #to get a vector output as input to the fully connected layer ,neurons must be flattened not spatial\n",
        "\n",
        "         x=self.fc1(x) # 1st fully connected layer for classification\n",
        "         x=F.relu(x)\n",
        "         #print(x.shape)\n",
        "\n",
        "         #x=self.dropout2(x)\n",
        "\n",
        "         x=self.fc2(x)\n",
        "         x=F.relu(x)\n",
        "         #print(x.shape)\n",
        "\n",
        "         #x=self.dropout3(x)\n",
        "\n",
        "         x=self.fc3(x)\n",
        "\n",
        "         #print(x.shape)\n",
        "         output=F.log_softmax(x,dim=1) #log softmax activation function to get probablity of each class\n",
        "         return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDcVOBZfuiGS"
      },
      "source": [
        "# Accuracy calculation\n",
        "def calculate_accuracy(model,train_dataloader,val_dataloader,test_dataloader):\n",
        "\n",
        "  from sklearn.metrics import f1_score\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "  n_correct_train = 0\n",
        "  n_samples_train = 0\n",
        "  for images, labels in train_dataloader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = model(images)\n",
        "    _, predicted_train = torch.max(outputs, 1)\n",
        "    n_samples_train += labels.size(0)\n",
        "    n_correct_train+= (predicted_train == labels).sum().item()\n",
        "        \n",
        "\n",
        "  acc_train = 100.0 * n_correct_train / n_samples_train\n",
        "  f1score_train = f1_score(labels.to(device), predicted_train.to(device), average='micro')\n",
        "  print(f'training Accuracy : {acc_train} %')\n",
        "  print(f'training F1 score: {round(f1score_train, 5)}')\n",
        "\n",
        "\n",
        "  n_correct_val = 0\n",
        "  n_samples_val = 0\n",
        "  for images, labels in val_dataloader:\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    outputs = model(images)\n",
        "    _, predicted_val = torch.max(outputs, 1)\n",
        "    n_samples_val += labels.size(0)\n",
        "    n_correct_val+= (predicted_val == labels).sum().item()\n",
        "        \n",
        "\n",
        "  acc_val = 100.0 * n_correct_val / n_samples_val\n",
        "  f1score_val = f1_score(labels.to(device), predicted_val.to(device), average='micro')\n",
        "  print(f'validation Accuracy : {acc_val} %')\n",
        "  print(f'validation F1 score: {round(f1score_val, 5)}')\n",
        "\n",
        "\n",
        "\n",
        "  with torch.no_grad(): # To disable gradient calculation so memory consumption decreases \n",
        "    n_correct_test = 0\n",
        "    n_samples_test = 0\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted_test = torch.max(outputs, 1)\n",
        "        n_samples_test += labels.size(0)\n",
        "        n_correct_test += (predicted_test == labels).sum().item()\n",
        "        \n",
        "\n",
        "    acc_test = 100.0 * n_correct_test / n_samples_test\n",
        "    f1score_test = f1_score(labels.to(device), predicted_test.to(device), average='micro')\n",
        "    print(f'Testing Accuracy : {acc_test} %')\n",
        "    print(f' testing F1 score: {round(f1score_test, 5)}')\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4-aekt6p_Bv"
      },
      "source": [
        "# ModelRun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgmXL8bgqGLF"
      },
      "source": [
        "def Run_model(model,train_dataloader,val_dataloader,test_dataloader,optimizer):\n",
        "\n",
        "  model.train()\n",
        " \n",
        "  model.cuda()\n",
        "\n",
        "  \n",
        "  predictions=[]\n",
        "  labels=[]\n",
        "  loss_function= nn.CrossEntropyLoss() #loss_function\n",
        "\n",
        "  total_train_loss=0\n",
        "  total_val_loss=0\n",
        "  total_test_loss=0\n",
        "  \n",
        "  # training loop\n",
        "\n",
        "  for i,(images,label) in enumerate(train_dataloader):\n",
        "        images,label=images.cuda(),label.cuda()\n",
        "        optimizer.zero_grad()  #to zero gradients at start of each new batch since backward function accumulates gradients (we don’t want to mix up gradients between minibatches)\n",
        "        output = model(images) #gets the probability of each class in this batch\n",
        "        train_loss = loss_function(output, label) #compares output class probabilities of the 'model' and the labels in 'training_data_loader' and gets the loss for this batch\n",
        "        total_train_loss += train_loss.item()  # to get total loss of all batches\n",
        "        train_loss.backward() #backward pass of optimizer\n",
        "        optimizer.step() #to update weights and parameters of optimizer \n",
        "        \n",
        "        \n",
        "\n",
        " \n",
        "  #f1score_train = f1_score(labels.to(device), predicted_train, average='micro')\n",
        "\n",
        "\n",
        "  model.eval() #it switches off some layers/parts of the model that behave differently during evaluation time,for ex Dropouts Layer\n",
        "  \n",
        "  # validation loop\n",
        "  for i,(images, label) in enumerate(val_dataloader):\n",
        "        images,label=images.cuda(),label.cuda()\n",
        "        output = model(images)\n",
        "        val_loss = loss_function(output, label)\n",
        "        total_val_loss += val_loss.item()#*data.size(0)\n",
        "\n",
        "  # testing loop\n",
        "  for i,(images, label) in enumerate(test_dataloader):\n",
        "        images,label=images.cuda(),label.cuda()\n",
        "        output = model(images)\n",
        "        test_loss = loss_function(output, label)\n",
        "        total_test_loss += test_loss.item()#*data.size(0)\n",
        "\n",
        "\n",
        "  #average losses\n",
        "  average_train_loss = total_train_loss/len(train_dataloader.sampler)\n",
        "  average_valid_loss = total_val_loss/len(val_dataloader.sampler)\n",
        "  average_test_loss = total_test_loss/len(test_dataloader.sampler)\n",
        "\n",
        "  return average_train_loss,average_train_loss,average_test_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8JeRbx9kF1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98e85da-9088-4fb2-b67f-217485d1d75c"
      },
      "source": [
        "#initializing the models\n",
        "model1=CnnManyLayers() \n",
        "\n",
        "\n",
        "epochs=5 #number of epochs\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model1.parameters(), lr=0.0001) #for optimizing parameters\n",
        "schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=5, threshold=1e-4) #it prevents conversion that happens due to over useless training\n",
        "\n",
        "\n",
        "\n",
        "for i in range (epochs):\n",
        " \n",
        "  trainloss,valloss,testloss=Run_model(model1,trainloader,valloader,testloader,optimizer )\n",
        "\n",
        "  print(f'Epoch : {i+1}')\n",
        "  print(f'Training Loss: {trainloss}')\n",
        "  print(f'Validation Loss: {valloss}')\n",
        "  print(f'Test Loss: {testloss}')\n",
        "  calculate_accuracy(model1,trainloader,valloader,testloader )\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1\n",
            "Training Loss: 0.02099324558716724\n",
            "Validation Loss: 0.02099324558716724\n",
            "Test Loss: 0.010235875697567972\n",
            "training Accuracy : 87.4356333676622 %\n",
            "training F1 score: 0.0\n",
            "validation Accuracy : 87.88659793814433 %\n",
            "validation F1 score: 1.0\n",
            "Testing Accuracy : 87.4356333676622 %\n",
            " testing F1 score: 1.0\n",
            "Epoch : 2\n",
            "Training Loss: 0.009383140457298517\n",
            "Validation Loss: 0.009383140457298517\n",
            "Test Loss: 0.0113816592718292\n",
            "training Accuracy : 84.24304840370752 %\n",
            "training F1 score: 1.0\n",
            "validation Accuracy : 84.27835051546391 %\n",
            "validation F1 score: 0.75\n",
            "Testing Accuracy : 84.24304840370752 %\n",
            " testing F1 score: 1.0\n",
            "Epoch : 3\n",
            "Training Loss: 0.0069320878104127395\n",
            "Validation Loss: 0.0069320878104127395\n",
            "Test Loss: 0.025302218533528043\n",
            "training Accuracy : 72.2622725712324 %\n",
            "training F1 score: 1.0\n",
            "validation Accuracy : 70.36082474226804 %\n",
            "validation F1 score: 0.5\n",
            "Testing Accuracy : 72.2622725712324 %\n",
            " testing F1 score: 0.0\n",
            "Epoch : 4\n",
            "Training Loss: 0.008058461927803386\n",
            "Validation Loss: 0.008058461927803386\n",
            "Test Loss: 0.004681033556440054\n",
            "training Accuracy : 94.57603844833505 %\n",
            "training F1 score: 1.0\n",
            "validation Accuracy : 93.29896907216495 %\n",
            "validation F1 score: 1.0\n",
            "Testing Accuracy : 94.57603844833505 %\n",
            " testing F1 score: 1.0\n",
            "Epoch : 5\n",
            "Training Loss: 0.00475565430733327\n",
            "Validation Loss: 0.00475565430733327\n",
            "Test Loss: 0.004475419902889499\n",
            "training Accuracy : 94.67902506007552 %\n",
            "training F1 score: 1.0\n",
            "validation Accuracy : 94.0721649484536 %\n",
            "validation F1 score: 1.0\n",
            "Testing Accuracy : 94.67902506007552 %\n",
            " testing F1 score: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H6PTfH9gVeS"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw5wxUuqNPYR",
        "outputId": "0f4f74db-56f6-44b5-bd38-44c76743b471"
      },
      "source": [
        "model2=CnnFewLayers()\n",
        "for i in range (epochs):\n",
        " \n",
        "  trainloss2,valloss2,testloss2=Run_model(model2,trainloader,valloader,testloader,optimizer )\n",
        "\n",
        "  print(f'Epoch : {i+1}')\n",
        "  print(f'Training Loss 2: {trainloss2}')\n",
        "  print(f'Validation Loss 2: {valloss2}')\n",
        "  print(f'Test Loss 2: {testloss2}')\n",
        "  calculate_accuracy(model2,trainloader,valloader,testloader )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1\n",
            "Training Loss 2: 0.034683122530124295\n",
            "Validation Loss 2: 0.034683122530124295\n",
            "Test Loss 2: 0.034682730281226135\n",
            "training Accuracy : 34.5005149330587 %\n",
            "training F1 score: 1.0\n",
            "validation Accuracy : 34.5360824742268 %\n",
            "validation F1 score: 0.5\n",
            "Testing Accuracy : 34.5005149330587 %\n",
            " testing F1 score: 0.0\n",
            "Epoch : 2\n",
            "Training Loss 2: 0.03468263177907716\n",
            "Validation Loss 2: 0.03468263177907716\n",
            "Test Loss 2: 0.0346957380076761\n",
            "training Accuracy : 34.5005149330587 %\n",
            "training F1 score: 1.0\n",
            "validation Accuracy : 34.5360824742268 %\n",
            "validation F1 score: 0.5\n",
            "Testing Accuracy : 34.5005149330587 %\n",
            " testing F1 score: 0.0\n",
            "Epoch : 3\n",
            "Training Loss 2: 0.03468253831048213\n",
            "Validation Loss 2: 0.03468253831048213\n",
            "Test Loss 2: 0.03469625458126415\n",
            "training Accuracy : 34.5005149330587 %\n",
            "training F1 score: 1.0\n",
            "validation Accuracy : 34.5360824742268 %\n",
            "validation F1 score: 1.0\n",
            "Testing Accuracy : 34.5005149330587 %\n",
            " testing F1 score: 0.0\n",
            "Epoch : 4\n",
            "Training Loss 2: 0.034682353378529\n",
            "Validation Loss 2: 0.034682353378529\n",
            "Test Loss 2: 0.03468279682235377\n",
            "training Accuracy : 34.5005149330587 %\n",
            "training F1 score: 0.0\n",
            "validation Accuracy : 34.5360824742268 %\n",
            "validation F1 score: 0.75\n",
            "Testing Accuracy : 34.5005149330587 %\n",
            " testing F1 score: 1.0\n",
            "Epoch : 5\n",
            "Training Loss 2: 0.034688001107891275\n",
            "Validation Loss 2: 0.034688001107891275\n",
            "Test Loss 2: 0.03469729898232101\n",
            "training Accuracy : 34.5005149330587 %\n",
            "training F1 score: 0.0\n",
            "validation Accuracy : 34.5360824742268 %\n",
            "validation F1 score: 0.5\n",
            "Testing Accuracy : 34.5005149330587 %\n",
            " testing F1 score: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owzHVgBbdv9w"
      },
      "source": [
        "res_model_not_pretrained = torchvision.models.resnet18(pretrained=False)\n",
        "res_model_pretrained = torchvision.models.resnet18(pretrained=True)\n",
        "#densenet_model = models.densenet161()\n",
        "\n",
        "\n",
        "#res_model = nn.Linear(num_fltrs , 3)\n",
        "#ens_model = torchvision.models.densenet(pretrained=True)\n",
        "#ins_model = torchvision.models.inception_v3()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCiV9Q1DhoVz"
      },
      "source": [
        "**Resnet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwIfUQMMhLK1"
      },
      "source": [
        "#from sklearn.metrics import f1_score\n",
        "def train_model(res_model):\n",
        "  #calculate number of  in features \n",
        "  num_fltrs = res_model.fc.in_features\n",
        "  # Freeze all layers\n",
        "  for param in res_model.parameters():\n",
        "    param.requires_grad = False\n",
        "      \n",
        "\n",
        "  # Replace the last layer\n",
        "  # Parameters of newly constructed modules have requires_grad=True by default len(trainset.classes\n",
        "  #res_model.classifier = nn.Linear(25088, 3)\n",
        "  #Tensor for 'out' is on CPU, Tensor for argument #1 'self' is on CPU, but expected them to be on GPU (while checking arguments for addmm)\n",
        "  res_model.fc = torch.nn.Linear(num_fltrs, out_features = 3)\n",
        "  #res_model = res_model.to('cuda:0')\n",
        "  #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # Optimize only the parameters of ''classifier'\n",
        "  optimizer = torch.optim.Adam(res_model.parameters(), lr=1e-3)\n",
        "  #crit = nn.CrossEntropyLoss()\n",
        "  #if (dataloaders == \"train\"):\n",
        "  #res_model.train()\n",
        "  res_model = res_model.cuda()\n",
        "\n",
        "  # Loss and optimizer\n",
        "  num_epochs = 15\n",
        "  print_every = 10\n",
        "  #res_model_a = res_model\n",
        " # res_model_b = res_model\n",
        "\n",
        "  # Optimization loop\n",
        "  for epoch in range(num_epochs):\n",
        "      print('Start of epoch', epoch)\n",
        "      total_loss = 0\n",
        "      total_examples = 0\n",
        "      total_correct = 0\n",
        "      num_correct = 0\n",
        "      H = 0\n",
        "      N = 0\n",
        "      A = 0\n",
        "      B = 0\n",
        "\n",
        "      \n",
        "      for iteration, (x, Y) in enumerate(trainloader):\n",
        "          res_model.train(True)\n",
        "         # res_model_a = res_model.cuda()\n",
        "          # Forward\n",
        "          x = x.cuda()\n",
        "          Y = Y.cuda()\n",
        "          scores = res_model(x)\n",
        "\n",
        "          # Loss\n",
        "          loss = torch.nn.functional.cross_entropy(scores, Y)\n",
        "\n",
        "        \n",
        "          \n",
        "          # Accuracy\n",
        "          preds = torch.argmax(scores, dim=1)\n",
        "          num_correct = torch.sum(preds == Y)\n",
        "          #f1score = f1_score(y, preds, average='micro')\n",
        "          \n",
        "          # Print\n",
        "          total_loss += loss.item()\n",
        "          total_correct += num_correct.item()\n",
        "          total_examples += len(x)\n",
        "          if iteration % print_every == 0:\n",
        "              H += (total_loss / total_examples)\n",
        "              N += (total_correct/total_examples)\n",
        "              #print('trainLoss ='+ str(total_loss / total_examples))\n",
        "              #print('trainAcc ='+ str(total_correct/total_examples))\n",
        "             # print(f1score)\n",
        "          \n",
        "          # Backward\n",
        "          loss.backward()\n",
        "          \n",
        "          # Update\n",
        "          with torch.no_grad():\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "      total_loss = 0\n",
        "      total_examples = 0\n",
        "      total_correct = 0\n",
        "      num_correct = 0\n",
        "      H = \"{:.4f}\".format(H/10)\n",
        "      N = \"{:.4f}\".format(N/10)\n",
        "      print('trainLoss ='+ str(H))\n",
        "      print('trainAcc ='+ str(N))\n",
        "      for iteration, (X, y) in enumerate(valloader):\n",
        "          res_model.eval\n",
        "          #res_model = res_model.cuda()\n",
        "          # Forward\n",
        "          X = X.cuda()\n",
        "          y = y.cuda()\n",
        "          scores = res_model(X)\n",
        "\n",
        "          # Loss\n",
        "          loss = torch.nn.functional.cross_entropy(scores, y)\n",
        "\n",
        "        \n",
        "          \n",
        "          # Accuracy\n",
        "          preds = torch.argmax(scores, dim=1)\n",
        "          num_correct = torch.sum(preds == y)\n",
        "          \n",
        "          # Print\n",
        "          total_loss += loss.item()\n",
        "          total_correct += num_correct.item()\n",
        "          total_examples += len(valloader)\n",
        "          if iteration % print_every == 0:\n",
        "              A += (total_loss / total_examples)\n",
        "              B += (total_correct/total_examples)\n",
        "            #print('valLoss ='+ str(total_loss / total_examples))\n",
        "            #print('valAcc ='+ str(total_correct/total_examples))\n",
        "          \n",
        "          # Update\n",
        "          with torch.no_grad():\n",
        "              #optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "              \n",
        "      A = \"{:.4f}\".format(A/10)\n",
        "      B = \"{:.4f}\".format(B/10)\n",
        "      print('valLoss ='+ str(A))\n",
        "      print('valAcc ='+ str(B))\n",
        "      print('......................')\n",
        "              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdfae84DV8BZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e171bd0-04d9-400d-9f19-7e8a4c4b8d2c"
      },
      "source": [
        "#train_model(res_model_not_pretrained)\n",
        "train_model(res_model_pretrained)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 0\n",
            "trainLoss =0.0216\n",
            "trainAcc =0.7225\n",
            "valLoss =0.0037\n",
            "valAcc =0.4629\n",
            "......................\n",
            "Start of epoch 1\n",
            "trainLoss =0.0081\n",
            "trainAcc =0.9216\n",
            "valLoss =0.0026\n",
            "valAcc =0.4776\n",
            "......................\n",
            "Start of epoch 2\n",
            "trainLoss =0.0069\n",
            "trainAcc =0.9206\n",
            "valLoss =0.0019\n",
            "valAcc =0.4748\n",
            "......................\n",
            "Start of epoch 3\n",
            "trainLoss =0.0055\n",
            "trainAcc =0.9476\n",
            "valLoss =0.0024\n",
            "valAcc =0.4650\n",
            "......................\n",
            "Start of epoch 4\n",
            "trainLoss =0.0048\n",
            "trainAcc =0.9499\n",
            "valLoss =0.0023\n",
            "valAcc =0.4713\n",
            "......................\n",
            "Start of epoch 5\n",
            "trainLoss =0.0050\n",
            "trainAcc =0.9469\n",
            "valLoss =0.0015\n",
            "valAcc =0.4741\n",
            "......................\n",
            "Start of epoch 6\n",
            "trainLoss =0.0047\n",
            "trainAcc =0.9452\n",
            "valLoss =0.0015\n",
            "valAcc =0.4832\n",
            "......................\n",
            "Start of epoch 7\n",
            "trainLoss =0.0041\n",
            "trainAcc =0.9547\n",
            "valLoss =0.0019\n",
            "valAcc =0.4741\n",
            "......................\n",
            "Start of epoch 8\n",
            "trainLoss =0.0046\n",
            "trainAcc =0.9432\n",
            "valLoss =0.0014\n",
            "valAcc =0.4839\n",
            "......................\n",
            "Start of epoch 9\n",
            "trainLoss =0.0041\n",
            "trainAcc =0.9587\n",
            "valLoss =0.0017\n",
            "valAcc =0.4748\n",
            "......................\n",
            "Start of epoch 10\n",
            "trainLoss =0.0044\n",
            "trainAcc =0.9478\n",
            "valLoss =0.0036\n",
            "valAcc =0.4497\n",
            "......................\n",
            "Start of epoch 11\n",
            "trainLoss =0.0036\n",
            "trainAcc =0.9592\n",
            "valLoss =0.0017\n",
            "valAcc =0.4713\n",
            "......................\n",
            "Start of epoch 12\n",
            "trainLoss =0.0040\n",
            "trainAcc =0.9601\n",
            "valLoss =0.0021\n",
            "valAcc =0.4727\n",
            "......................\n",
            "Start of epoch 13\n",
            "trainLoss =0.0036\n",
            "trainAcc =0.9612\n",
            "valLoss =0.0017\n",
            "valAcc =0.4755\n",
            "......................\n",
            "Start of epoch 14\n",
            "trainLoss =0.0032\n",
            "trainAcc =0.9637\n",
            "valLoss =0.0024\n",
            "valAcc =0.4699\n",
            "......................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m433FEZFbh0v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEcGY4rgquz6"
      },
      "source": [
        "\n",
        "def train_model_val(res_model,valloaders):\n",
        "  num_fltrs = res_model.fc.in_features\n",
        "  # Freeze all layers\n",
        "  for param in res_model.parameters():\n",
        "    param.requires_grad = False\n",
        "      \n",
        "\n",
        "  # Replace the last layer\n",
        "  # Parameters of newly constructed modules have requires_grad=True by default len(trainset.classes\n",
        "  #res_model.classifier = nn.Linear(25088, 3)\n",
        "  #Tensor for 'out' is on CPU, Tensor for argument #1 'self' is on CPU, but expected them to be on GPU (while checking arguments for addmm)\n",
        "  res_model.fc = torch.nn.Linear(num_fltrs, out_features = 3)\n",
        "  #res_model = res_model.to('cuda:0')\n",
        "  #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # Optimize only the parameters of ''classifier'\n",
        "  optimizer = torch.optim.Adam(res_model.parameters(), lr=1e-3)\n",
        "  #crit = nn.CrossEntropyLoss()\n",
        "  #if (dataloaders == \"train\"):\n",
        "  res_model = res_model.cuda()\n",
        "\n",
        "  # Loss and optimizer\n",
        "  num_epochs = 5\n",
        "  print_every = 10\n",
        "\n",
        "  # Optimization loop\n",
        "  for epoch in range(num_epochs):\n",
        "      print('Start of epoch', epoch)\n",
        "      total_loss = 0\n",
        "      total_examples = 0\n",
        "      total_correct = 0\n",
        "      num_correct = 0\n",
        "\n",
        "      \n",
        "      for iteration, (X, y) in enumerate(valloader):\n",
        "          # Forward\n",
        "          X = X.cuda()\n",
        "          y = y.cuda()\n",
        "          scores = res_model(X)\n",
        "\n",
        "          # Loss\n",
        "          loss = torch.nn.functional.cross_entropy(scores, y)\n",
        "\n",
        "        \n",
        "          \n",
        "          # Accuracy\n",
        "          preds = torch.argmax(scores, dim=1)\n",
        "          num_correct = torch.sum(preds == y)\n",
        "          \n",
        "          # Print\n",
        "          total_loss += loss.item()\n",
        "          total_correct += num_correct.item()\n",
        "          total_examples += len(X)\n",
        "          if iteration % print_every == 0:\n",
        "              print(total_loss / total_examples)\n",
        "          \n",
        "          \n",
        "          # Update\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hth3yRy3rsFw",
        "outputId": "2f66668a-0fa3-4c93-d402-e65667a6ac2d"
      },
      "source": [
        "train_model_val(res_model_not_pretrained,valloader)\n",
        "train_model_val(res_model_pretrained,valloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 0\n",
            "0.040929269045591354\n",
            "0.03894013301892714\n",
            "Start of epoch 1\n",
            "0.03861590474843979\n",
            "0.03879295255650173\n",
            "Start of epoch 2\n",
            "0.040694475173950195\n",
            "0.039072025905955925\n",
            "Start of epoch 3\n",
            "0.035858556628227234\n",
            "0.03878326307643543\n",
            "Start of epoch 4\n",
            "0.036722078919410706\n",
            "0.0388918783177029\n",
            "Start of epoch 0\n",
            "0.04677983373403549\n",
            "0.04158848862756382\n",
            "Start of epoch 1\n",
            "0.04442312568426132\n",
            "0.042017253962430084\n",
            "Start of epoch 2\n",
            "0.040883325040340424\n",
            "0.041240579363974655\n",
            "Start of epoch 3\n",
            "0.034565869718790054\n",
            "0.0409862141717564\n",
            "Start of epoch 4\n",
            "0.04021246358752251\n",
            "0.041749436408281326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "y6zVk68CuggL",
        "outputId": "e6c5aa23-59d2-482f-f358-9d2749cf76a3"
      },
      "source": [
        "# Freeze all layers\n",
        "for param in res_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "\n",
        "# Replace the last layer\n",
        "# Parameters of newly constructed modules have requires_grad=True by default len(trainset.classes\n",
        "#res_model.classifier = nn.Linear(25088, 3)\n",
        "#Tensor for 'out' is on CPU, Tensor for argument #1 'self' is on CPU, but expected them to be on GPU (while checking arguments for addmm)\n",
        "res_model.fc = torch.nn.Linear(num_fltrs, out_features = 3)\n",
        "#res_model = res_model.to('cuda:0')\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Optimize only the parameters of ''classifier'\n",
        "optimizer = torch.optim.Adam(res_model.parameters(), lr=1e-3)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "res_model.train()\n",
        "res_model = res_model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "num_epochs = 5\n",
        "print_every = 10\n",
        "\n",
        "# Optimization loop\n",
        "for epoch in range(num_epochs):\n",
        "    print('Start of epoch', epoch)\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    total_correct = 0\n",
        "    num_correct = 0\n",
        "\n",
        "    \n",
        "    for iteration, (X, y) in enumerate(train_loader):\n",
        "        # Forward\n",
        "        X = X.cuda\n",
        "        y = y.cuda\n",
        "        scores = res_model(X)\n",
        "\n",
        "       # X = X.view(-1 , 331*331) # da b3d el resize msh 3arf malo \n",
        "        optimizer.zero_grad()\n",
        "        print(X.shape)\n",
        "        \n",
        "        print(\"1\")\n",
        "\n",
        "        # Loss\n",
        "        loss = crit(scores, y)\n",
        "\n",
        "        # Accuracy\n",
        "        _, predicted = torch.max(scores.data, 1)\n",
        "        total_correct += y.size(0)\n",
        "        num_correct += (predicted == y).sum()\n",
        "        # preds = torch.argmax(scores, dim=1)\n",
        "        # num_correct = torch.sum(preds == y)\n",
        "        \n",
        "        # Print\n",
        "        total_loss += loss.item()\n",
        "        # total_correct += num_correct.item()\n",
        "        # total_examples += len(X)\n",
        "        if iteration % print_every == 0:\n",
        "            print(total_loss / total_examples, num_correct/total_correct)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update\n",
        "        with torch.no_grad():\n",
        "            optimizer.step()\n",
        "            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-6d17c10ad251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Freeze all layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'parameters'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT-BrYwKiDWH"
      },
      "source": [
        "**DenseNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akxQ0MZhib4b"
      },
      "source": [
        "# Freeze all layers\n",
        "for param in dens_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "dens_model.classifier = nn.Linear(25088, len(trainset.classes))  \n",
        "\n",
        "# Optimize only the parameters of ''classifier'\n",
        "optimizer = torch.optim.Adam(dens_model.classifier.parameters(), lr=1e-3)\n",
        "\n",
        "dens_model.train()\n",
        "dens_model = dens_model.cuda()\n",
        "\n",
        "# Loss and optimizer\n",
        "num_epochs = 5\n",
        "print_every = 10\n",
        "\n",
        "# Optimization loop\n",
        "for epoch in range(num_epochs):\n",
        "    print('Start of epoch', epoch)\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    total_correct = 0\n",
        "    \n",
        "    for iteration, (X, y) in enumerate(train_loader):\n",
        "        # Forward\n",
        "        X = X.cuda()    \n",
        "        y = y.cuda()\n",
        "        scores = dens_model(X)\n",
        "\n",
        "        # Loss\n",
        "        loss = torch.nn.functional.cross_entropy(scores, y)\n",
        "\n",
        "        # Accuracy\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        num_correct = torch.sum(preds == y)\n",
        "        \n",
        "        # Print\n",
        "        total_loss += loss.item()\n",
        "        total_correct += num_correct.item()\n",
        "        total_examples += len(X)\n",
        "        if iteration % print_every == 0:\n",
        "            print(total_loss / total_examples, total_correct / total_examples)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update\n",
        "        with torch.no_grad():\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYzMyuz_i2eJ"
      },
      "source": [
        "**Inspection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8a-fATji88C"
      },
      "source": [
        "# Freeze all layers\n",
        "for param in ins_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "ins_model.classifier = nn.Linear(25088, len(trainset.classes))  \n",
        "\n",
        "# Optimize only the parameters of ''classifier'\n",
        "optimizer = torch.optim.Adam(ins_model.classifier.parameters(), lr=1e-3)\n",
        "\n",
        "ins_model.train()\n",
        "ins_model = ins_model.cuda()\n",
        "\n",
        "# Loss and optimizer\n",
        "num_epochs = 5\n",
        "print_every = 10\n",
        "\n",
        "# Optimization loop\n",
        "for epoch in range(num_epochs):\n",
        "    print('Start of epoch', epoch)\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "    total_correct = 0\n",
        "    \n",
        "    for iteration, (X, y) in enumerate(train_loader):\n",
        "        # Forward\n",
        "        X = X.cuda()    \n",
        "        y = y.cuda()\n",
        "        scores = ins_model(X)\n",
        "\n",
        "        # Loss\n",
        "        loss = torch.nn.functional.cross_entropy(scores, y)\n",
        "\n",
        "        # Accuracy\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        num_correct = torch.sum(preds == y)\n",
        "        \n",
        "        # Print\n",
        "        total_loss += loss.item()\n",
        "        total_correct += num_correct.item()\n",
        "        total_examples += len(X)\n",
        "        if iteration % print_every == 0:\n",
        "            print(total_loss / total_examples, total_correct / total_examples)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update\n",
        "        with torch.no_grad():\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}